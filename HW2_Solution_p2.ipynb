{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI544-HW2_p2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgWl-wYxITJm"
      },
      "source": [
        "## 5. Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iQ2RKxpQImKw",
        "outputId": "f0d9ba5f-79f7-4732-9a64-987e005c5731"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWezOyCRJ83d"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "file_path1 = '/content/drive/MyDrive/Colab Notebooks/train_datawithout3.json'\n",
        "file_path2 = '/content/drive/MyDrive/Colab Notebooks/test_datawithout3.json'\n",
        "\n",
        "reviews_train_lemmatize_split = json.load(open(file_path1))\n",
        "reviews_test_lemmatize_split = json.load(open(file_path2))\n",
        "\n",
        "file_path3 = '/content/drive/MyDrive/Colab Notebooks/train_data.json'\n",
        "file_path4 = '/content/drive/MyDrive/Colab Notebooks/test_data.json'\n",
        "\n",
        "reviews_train_lemmatize_split1 = json.load(open(file_path3))\n",
        "reviews_test_lemmatize_split1 = json.load(open(file_path4))\n",
        "\n",
        "y_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bin_label_train.csv')\n",
        "y_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bin_label_test.csv')\n",
        "y_train1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ter_label_train.csv')\n",
        "y_test1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ter_label_test.csv')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_nm4xpVJSxf"
      },
      "source": [
        "### 5.1 Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCvB_Z6FI9eh"
      },
      "source": [
        "def word_embed_2(review, m):\n",
        "    doc = []\n",
        "    n = 0 \n",
        "    for r in review:\n",
        "        if r in m:\n",
        "            n += 1\n",
        "            doc.append(m[r])\n",
        "            if n==50:\n",
        "                break\n",
        "    while n!=50:\n",
        "        doc.append(np.zeros(300))\n",
        "        n += 1\n",
        "    return doc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpVjqt-nRcsc"
      },
      "source": [
        "Google Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG9ZbHykJfm-"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSFHT-C6JwTY"
      },
      "source": [
        "import numpy as np\n",
        "# deal with dataset without class 3\n",
        "x_train_pretrain_bin_50 = []\n",
        "x_test_pretrain_bin_50 = []\n",
        "for review in reviews_train_lemmatize_split:\n",
        "    x_train_pretrain_bin_50.append(word_embed_2(review, wv))\n",
        "for review in reviews_test_lemmatize_split:\n",
        "    x_test_pretrain_bin_50.append(word_embed_2(review, wv))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAQxM4Yv57bv"
      },
      "source": [
        "# deal with dataset with class 3\n",
        "x_train_pretrain_ter_50 = []\n",
        "x_test_pretrain_ter_50 = []\n",
        "for review in reviews_train_lemmatize_split1:\n",
        "    x_train_pretrain_ter_50.append(word_embed_2(review, wv))\n",
        "    \n",
        "for review in reviews_test_lemmatize_split1:\n",
        "    x_test_pretrain_ter_50.append(word_embed_2(review, wv))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qupW3rW03yOo"
      },
      "source": [
        "my word2vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk3mvXgg3vEA"
      },
      "source": [
        "import gensim\n",
        "model = gensim.models.Word2Vec.load('/content/drive/MyDrive/Colab Notebooks/w2v')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhPLHgOX36_M"
      },
      "source": [
        "x_train_my_bin_50 = []\n",
        "x_test_my_bin_50 = []\n",
        "for review in reviews_train_lemmatize_split:\n",
        "    x_train_my_bin_50.append(word_embed_2(review, model.wv))\n",
        "    \n",
        "for review in reviews_test_lemmatize_split:\n",
        "    x_test_my_bin_50.append(word_embed_2(review, model.wv))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq9C24tH39YG"
      },
      "source": [
        "# deal with dataset with class 3\n",
        "import numpy as np\n",
        "x_train_my_ter_50 = []\n",
        "x_test_my_ter_50 = []\n",
        "for review in reviews_train_lemmatize_split1:\n",
        "    x_train_my_ter_50.append(word_embed_2(review, model.wv))\n",
        "    \n",
        "for review in reviews_test_lemmatize_split1:\n",
        "    x_test_my_ter_50.append(word_embed_2(review, model.wv))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feYGG-X56Hyb"
      },
      "source": [
        "### 5.2 DataLoader create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8sSAc-Af7O9"
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW7WIMb_Vh3J"
      },
      "source": [
        "class Reviews(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, index):\n",
        "        embeds = self.x[index]\n",
        "        label = self.y[index]\n",
        "        \n",
        "        return embeds, label"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOSxtZNimaBK"
      },
      "source": [
        "binary_train_y = np.squeeze(y_train.values)\n",
        "binary_test_y = np.squeeze(y_test.values)\n",
        "binary_train_RNN = Reviews(x_train_pretrain_bin_50, binary_train_y)\n",
        "binary_test_RNN = Reviews(x_test_pretrain_bin_50, binary_test_y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqL51hl1Vi60"
      },
      "source": [
        "\n",
        "def pad_data(batch):\n",
        "    X = [torch.tensor(x[0], dtype=torch.float32) for x in batch]\n",
        "    Y = [x[1] for x in batch]\n",
        "    X_len = [len(x[0]) for x in batch]\n",
        "    X_pad = pad_sequence(X, batch_first=True, padding_value=0)\n",
        "    return X_pad, Y, X_len"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M16ioiknVlpi"
      },
      "source": [
        "\n",
        "num_workers = 0\n",
        "batch_size = 64\n",
        "valid_size = 0.2\n",
        "num_train = len(x_train_pretrain_bin_50)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# sample the train and validation batch\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "# DataLoaders \n",
        "binary_train_dataloader_RNN = DataLoader(binary_train_RNN, sampler=train_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_valid_dataloader_RNN = DataLoader(binary_train_RNN, sampler=valid_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_test_dataloader_RNN = DataLoader(binary_test_RNN, batch_size=batch_size,collate_fn=pad_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0rKvetFGJfp"
      },
      "source": [
        "# deal with dataset with class 3 use Google word2vec\n",
        "ternary_train_y = np.squeeze(y_train1.values)\n",
        "ternary_train_y[ternary_train_y==3]=0\n",
        "ternary_test_y = np.squeeze(y_test1.values)\n",
        "ternary_train_y[ternary_train_y==3]=0\n",
        "ternary_train_RNN = Reviews(x_train_pretrain_ter_50, ternary_train_y)\n",
        "ternary_test_RNN = Reviews(x_test_pretrain_ter_50, ternary_test_y)\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 64\n",
        "valid_size = 0.2\n",
        "num_train = len(x_train_pretrain_ter_50)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "ternary_train_dataloader_RNN = DataLoader(ternary_train_RNN, sampler=train_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "ternary_valid_dataloader_RNN = DataLoader(ternary_train_RNN, sampler=valid_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "ternary_test_dataloader_RNN = DataLoader(ternary_test_RNN, batch_size=batch_size,collate_fn=pad_data)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zde-0SCrvtzE"
      },
      "source": [
        "## 5.3 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDKuR_4y4IW2"
      },
      "source": [
        "### (a) RNN\n",
        "\n",
        "#### (a)-1 Binary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHIurDpUkC-L"
      },
      "source": [
        "Google Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFJVok5DfOxQ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw72N6xaWAoi"
      },
      "source": [
        "input_size = 300\n",
        "hidden_size = 50\n",
        "num_layers = 1\n",
        "sequence_length = 50\n",
        "output_size = 2\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # initize the hidden layer\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted = False)\n",
        "        out, _ = self.rnn(x_packed, h0)\n",
        "        out, out_lengths = pad_packed_sequence(out, batch_first=True)\n",
        "        out = out.reshape(out.shape[0],-1)\n",
        "        output = self.fc(out)\n",
        "        return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRTY27k_WC9C"
      },
      "source": [
        "rnn_bi_1 = RNN(input_size, hidden_size, num_layers, output_size)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn_bi_1.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg2tvhRuXDHh",
        "outputId": "89e94d6a-f346-41d5-f2a6-fd5b25482590"
      },
      "source": [
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  rnn_bi_1.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_bi_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  rnn_bi_1.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_bi_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [04:07<00:00,  8.09it/s]\n",
            "100%|██████████| 2000/2000 [03:38<00:00,  9.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.431976, Validation Loss: 1.553999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [04:06<00:00,  8.11it/s]\n",
            "100%|██████████| 2000/2000 [03:37<00:00,  9.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.390333, Validation Loss: 1.584250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [04:04<00:00,  8.20it/s]\n",
            "100%|██████████| 2000/2000 [03:35<00:00,  9.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.368653, Validation Loss: 1.402450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [04:01<00:00,  8.27it/s]\n",
            "100%|██████████| 2000/2000 [03:36<00:00,  9.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.349889, Validation Loss: 1.317528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [04:02<00:00,  8.25it/s]\n",
            "100%|██████████| 2000/2000 [03:39<00:00,  9.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.338538, Validation Loss: 1.249719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlUTWlYFp0L2"
      },
      "source": [
        "def test_acc(loader, model):\n",
        "  corr_cnt = 0\n",
        "  sampe_cnt = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for data, targets, lengths in loader:\n",
        "      scores = model(data, lengths)\n",
        "      _, pred = scores.max(1)\n",
        "      corr_cnt += sum(np.array(pred)==targets)\n",
        "      sampe_cnt += pred.size(0)\n",
        "  model.train()\n",
        "  return corr_cnt/sampe_cnt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6K2xrOK-DZs",
        "outputId": "591d8dcc-f2a0-4519-95fa-64213a63f538"
      },
      "source": [
        "print(\"accuracy of binary test set by using google word2vec:\", test_acc(binary_test_dataloader_RNN,rnn_bi_1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of binary test set by using google word2vec: 0.8449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzelPXnFkItm"
      },
      "source": [
        "my word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t76sPmLcS0TF"
      },
      "source": [
        "binary_train_y = np.squeeze(y_train.values)\n",
        "binary_test_y = np.squeeze(y_test.values)\n",
        "binary_train_RNN = Reviews(x_train_my_bin_50, binary_train_y)\n",
        "binary_test_RNN = Reviews(x_test_my_bin_50, binary_test_y)\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 64\n",
        "valid_size = 0.2\n",
        "num_train = len(x_train_my_bin_50)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# sample the train and validation batch\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# DataLoaders \n",
        "binary_train_dataloader_RNN = DataLoader(binary_train_RNN, sampler=train_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_valid_dataloader_RNN = DataLoader(binary_train_RNN, sampler=valid_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_test_dataloader_RNN = DataLoader(binary_test_RNN, batch_size=batch_size,collate_fn=pad_data)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-TVZZqGT2Ha"
      },
      "source": [
        "rnn_bi_2 = RNN(input_size, hidden_size, num_layers, output_size)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn_bi_2.parameters(), lr=0.001)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sQySijeHkS-W",
        "outputId": "4bdd917e-080d-4f73-c927-296de04f1367"
      },
      "source": [
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  rnn_bi_2.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_bi_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  rnn_bi_2.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_bi_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [03:58<00:00,  8.37it/s]\n",
            "100%|██████████| 2000/2000 [03:29<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.383375, Validation Loss: 1.341032\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [03:57<00:00,  8.41it/s]\n",
            "100%|██████████| 2000/2000 [03:45<00:00,  8.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.353551, Validation Loss: 1.322925\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [04:00<00:00,  8.33it/s]\n",
            "100%|██████████| 2000/2000 [03:32<00:00,  9.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.340988, Validation Loss: 1.238473\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [03:58<00:00,  8.40it/s]\n",
            "100%|██████████| 2000/2000 [03:30<00:00,  9.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.329981, Validation Loss: 1.185232\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [03:57<00:00,  8.43it/s]\n",
            "100%|██████████| 2000/2000 [03:31<00:00,  9.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.321567, Validation Loss: 1.216971\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j9c_AYTt3eRE",
        "outputId": "6565b894-9275-42f6-c0b5-bef76d29873d"
      },
      "source": [
        "print(\"accuracy of binary RNN by using my word2vec model:\", test_acc(binary_test_dataloader_RNN,rnn_bi_2))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of binary RNN by using my word2vec model: 0.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cscvcq-kavH"
      },
      "source": [
        "#### (a)-2 Ternary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raNfbaXPkdqr"
      },
      "source": [
        "Google Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lxM_MBueHRdk",
        "outputId": "f08e1309-a448-4492-8a42-e36a4195245c"
      },
      "source": [
        "rnn_ter_1 = RNN(input_size, hidden_size, num_layers, 3)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn_ter_1.parameters(), lr=0.001)\n",
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  rnn_ter_1.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_ter_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  rnn_ter_1.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_ter_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [05:02<00:00,  8.27it/s]\n",
            "100%|██████████| 2500/2500 [04:22<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.823216, Validation Loss: 3.090475\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [05:16<00:00,  7.90it/s]\n",
            "100%|██████████| 2500/2500 [04:25<00:00,  9.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.781442, Validation Loss: 2.997661\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [04:57<00:00,  8.40it/s]\n",
            "100%|██████████| 2500/2500 [04:22<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.765494, Validation Loss: 2.921576\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [04:56<00:00,  8.44it/s]\n",
            "100%|██████████| 2500/2500 [04:23<00:00,  9.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.749599, Validation Loss: 2.852113\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [05:09<00:00,  8.06it/s]\n",
            "100%|██████████| 2500/2500 [04:25<00:00,  9.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.738646, Validation Loss: 2.807690\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1rkkUj7nSggk",
        "outputId": "0ad8fb34-50ef-4b6f-8f99-b6558dbef693"
      },
      "source": [
        "print(\"accuracy of ternary RNN by using Google word2vec:\", test_acc(ternary_test_dataloader_RNN,rnn_ter_1))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of ternary RNN by using Google word2vec: 0.63382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKtt-I6gkkmm"
      },
      "source": [
        "my word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI5jBzyxS0-2"
      },
      "source": [
        "# deal with dataset with class 3\n",
        "ternary_train_y = np.squeeze(y_train1.values)\n",
        "ternary_train_y[ternary_train_y==3]=0\n",
        "ternary_test_y = np.squeeze(y_test1.values)\n",
        "ternary_train_y[ternary_train_y==3]=0\n",
        "ternary_train_RNN = Reviews(x_train_my_ter_50, ternary_train_y)\n",
        "ternary_test_RNN = Reviews(x_test_my_ter_50, ternary_test_y)\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 64\n",
        "valid_size = 0.2\n",
        "num_train = len(x_train_my_ter_50)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "ternary_train_dataloader_RNN = DataLoader(ternary_train_RNN, sampler=train_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "ternary_valid_dataloader_RNN = DataLoader(ternary_train_RNN, sampler=valid_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "ternary_test_dataloader_RNN = DataLoader(ternary_test_RNN, batch_size=batch_size,collate_fn=pad_data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "72wJwlETSok-",
        "outputId": "aea2c07a-2f90-424b-e1eb-b94c63248756"
      },
      "source": [
        "rnn_ter_2 = RNN(input_size, hidden_size, num_layers, 3)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn_ter_2.parameters(), lr=0.001)\n",
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  rnn_ter_2.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_ter_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  rnn_ter_2.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = rnn_ter_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [05:01<00:00,  8.29it/s]\n",
            "100%|██████████| 2500/2500 [04:24<00:00,  9.46it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 Training Loss: 0.785248, Validation Loss: 2.885084\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [04:58<00:00,  8.38it/s]\n",
            "100%|██████████| 2500/2500 [04:24<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.751025, Validation Loss: 2.837594\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [04:58<00:00,  8.38it/s]\n",
            "100%|██████████| 2500/2500 [04:23<00:00,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.736676, Validation Loss: 2.782840\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [04:59<00:00,  8.35it/s]\n",
            "100%|██████████| 2500/2500 [04:23<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.727402, Validation Loss: 2.755624\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [04:59<00:00,  8.34it/s]\n",
            "100%|██████████| 2500/2500 [04:25<00:00,  9.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.717901, Validation Loss: 2.719879\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L-HCjLOPYXF6",
        "outputId": "f6761910-7a2d-447a-8e24-49243183b8e7"
      },
      "source": [
        "print(\"accuracy of ternary RNN by using my word2vec:\", test_acc(ternary_test_dataloader_RNN,rnn_ter_2))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of ternary RNN by using my word2vec: 0.63576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R75m_hg4gVG"
      },
      "source": [
        "### (b) GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtG89CHgjHlk"
      },
      "source": [
        "#### (b)-1 Binary Classification\n",
        "\n",
        "Google Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZylX6er8l1Z"
      },
      "source": [
        "num_workers = 0\n",
        "batch_size = 128\n",
        "valid_size = 0.2\n",
        "num_train = len(x_train_pretrain_bin_50)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# sample the train and validation batch\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "# DataLoaders \n",
        "binary_train_dataloader_RNN = DataLoader(binary_train_RNN, sampler=train_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_valid_dataloader_RNN = DataLoader(binary_train_RNN, sampler=valid_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_test_dataloader_RNN = DataLoader(binary_test_RNN, batch_size=batch_size,collate_fn=pad_data)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtrDUP6w4jV8"
      },
      "source": [
        "input_size = 300\n",
        "hidden_size = 50\n",
        "num_layers = 1\n",
        "batch_size = 64\n",
        "sequence_length = 50\n",
        "output_size = 2\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * sequence_length, output_size)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # initize the hidden layer\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
        "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted = False)\n",
        "        out, _ = self.gru(x_packed, h0)\n",
        "        out, out_lengths = pad_packed_sequence(out, batch_first=True)\n",
        "        out = out.reshape(out.shape[0],-1)\n",
        "        output = self.fc(out)\n",
        "        return output"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "j6OHLp7L9StH",
        "outputId": "83c6f6dc-d499-49e8-b077-ebdeea264c0e"
      },
      "source": [
        "gru_bi_1 = GRU(input_size, hidden_size, num_layers, output_size)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru_bi_1.parameters(), lr=0.001)\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  gru_bi_1.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_bi_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  gru_bi_1.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_bi_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:37<00:00,  3.61it/s]\n",
            "100%|██████████| 1000/1000 [03:36<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.386307, Validation Loss: 1.337222\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:39<00:00,  3.57it/s]\n",
            "100%|██████████| 1000/1000 [03:37<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.315908, Validation Loss: 1.235545\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:39<00:00,  3.58it/s]\n",
            "100%|██████████| 1000/1000 [03:37<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.296045, Validation Loss: 1.101958\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:48<00:00,  3.46it/s]\n",
            "100%|██████████| 1000/1000 [03:35<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.279506, Validation Loss: 1.033319\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:39<00:00,  3.58it/s]\n",
            "100%|██████████| 1000/1000 [03:37<00:00,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.267655, Validation Loss: 0.997901\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX0_OIGsApvb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "09fdc438-1bb4-4ee4-8597-123e9a2291d7"
      },
      "source": [
        "print(\"accuracy of binary GRU by using google word2vec:\", test_acc(binary_test_dataloader_RNN,gru_bi_1))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of binary GRU by using google word2vec: 0.8768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8c914t0jTre"
      },
      "source": [
        "my word2vec binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wS_omGAjSSO"
      },
      "source": [
        "num_workers = 0\n",
        "batch_size = 128\n",
        "valid_size = 0.2\n",
        "num_train = len(x_train_my_bin_50)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(valid_size*num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# sample the train and validation batch\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "# DataLoaders \n",
        "binary_train_dataloader_RNN = DataLoader(binary_train_RNN, sampler=train_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_valid_dataloader_RNN = DataLoader(binary_train_RNN, sampler=valid_sampler, batch_size=batch_size,collate_fn=pad_data)\n",
        "binary_test_dataloader_RNN = DataLoader(binary_test_RNN, batch_size=batch_size,collate_fn=pad_data)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QisD5gsajiy6",
        "outputId": "f928de26-fa7f-468b-e0ea-16e17598c916"
      },
      "source": [
        "gru_bi_2 = GRU(input_size, hidden_size, num_layers, output_size)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru_bi_2.parameters(), lr=0.001)\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  gru_bi_2.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_bi_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  gru_bi_2.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(binary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_bi_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:38<00:00,  3.59it/s]\n",
            "100%|██████████| 1000/1000 [03:34<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.345819, Validation Loss: 1.165177\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:37<00:00,  3.60it/s]\n",
            "100%|██████████| 1000/1000 [03:34<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.298722, Validation Loss: 1.066820\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:35<00:00,  3.63it/s]\n",
            "100%|██████████| 1000/1000 [03:36<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.277815, Validation Loss: 0.966617\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:53<00:00,  3.41it/s]\n",
            "100%|██████████| 1000/1000 [03:35<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.260743, Validation Loss: 0.897642\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [04:37<00:00,  3.61it/s]\n",
            "100%|██████████| 1000/1000 [03:37<00:00,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.245677, Validation Loss: 0.854250\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MSL4fkvG4FYu",
        "outputId": "94fd4037-cc2a-47cd-eb85-f8245bddfb4e"
      },
      "source": [
        "print(\"accuracy of binary GRU by using my word2vec model:\", test_acc(binary_test_dataloader_RNN,gru_bi_2))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of binary GRU by using my word2vec model: 0.86945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYGd7j_z30uu"
      },
      "source": [
        "#### (b)-2 Ternary Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNozxCWC36A3"
      },
      "source": [
        "Google Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d91z5T7W38A9",
        "outputId": "9c820ea2-fef5-4f48-86b3-0a0ae83b2862"
      },
      "source": [
        "gru_ter_1 = GRU(input_size, hidden_size, num_layers, 3)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru_ter_1.parameters(), lr=0.001)\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  gru_ter_1.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_ter_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  gru_ter_1.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_ter_1(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:17<00:00,  6.61it/s]\n",
            "100%|██████████| 2500/2500 [04:45<00:00,  8.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.752938, Validation Loss: 2.762881\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:11<00:00,  6.73it/s]\n",
            "100%|██████████| 2500/2500 [04:38<00:00,  8.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.684669, Validation Loss: 2.650215\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:09<00:00,  6.77it/s]\n",
            "100%|██████████| 2500/2500 [04:43<00:00,  8.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.661130, Validation Loss: 2.600601\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:10<00:00,  6.74it/s]\n",
            "100%|██████████| 2500/2500 [04:55<00:00,  8.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.642333, Validation Loss: 2.440293\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:13<00:00,  6.70it/s]\n",
            "100%|██████████| 2500/2500 [04:39<00:00,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.626274, Validation Loss: 2.409096\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zOKk-owVTi5N",
        "outputId": "11e9c34a-6ced-44e6-e4a0-5fc53e615ee5"
      },
      "source": [
        "print(\"accuracy of ternary GRU by using Google word2vec:\", test_acc(ternary_test_dataloader_RNN,gru_ter_1))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of ternary GRU by using Google word2vec: 0.6524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJbg_c63-bN"
      },
      "source": [
        "my word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "GB19w0Nq3z2x",
        "outputId": "3bc5e0a2-1cf6-4399-a2d9-58e5e75e1a91"
      },
      "source": [
        "gru_ter_2 = GRU(input_size, hidden_size, num_layers, 3)\n",
        "crite = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gru_ter_2.parameters(), lr=0.001)\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  gru_ter_2.train()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_ter_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "  gru_ter_2.eval()\n",
        "  for batch_idx, (data, targets, lengths) in enumerate(tqdm(ternary_train_dataloader_RNN)):\n",
        "    targets = torch.tensor(targets, dtype=torch.long)\n",
        "    scores = gru_ter_2(data, lengths)\n",
        "    loss = crite(scores, targets)\n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "  train_loss = train_loss/(num_train - split)\n",
        "  valid_loss = valid_loss/split\n",
        "  print('Epoch: {} Training Loss: {:.6f}, Validation Loss: {:.6f}\\n'.format(\n",
        "      epoch+1, train_loss, valid_loss))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:20<00:00,  6.57it/s]\n",
            "100%|██████████| 2500/2500 [04:36<00:00,  9.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Training Loss: 0.739198, Validation Loss: 2.715519\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:03<00:00,  6.87it/s]\n",
            "100%|██████████| 2500/2500 [04:38<00:00,  8.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 Training Loss: 0.694046, Validation Loss: 2.580087\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:05<00:00,  6.85it/s]\n",
            "100%|██████████| 2500/2500 [04:39<00:00,  8.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 Training Loss: 0.674032, Validation Loss: 2.543447\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:21<00:00,  6.56it/s]\n",
            "100%|██████████| 2500/2500 [04:36<00:00,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 Training Loss: 0.658359, Validation Loss: 2.456954\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [06:00<00:00,  6.94it/s]\n",
            "100%|██████████| 2500/2500 [04:34<00:00,  9.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 Training Loss: 0.645589, Validation Loss: 2.398699\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3Sh51Q3qT5yP",
        "outputId": "a68f2a78-3e28-46f0-e14f-ba2dfb67ce58"
      },
      "source": [
        "print(\"accuracy of ternary GRU by using my word2vec model:\", test_acc(ternary_test_dataloader_RNN,gru_ter_2))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of ternary GRU by using my word2vec model: 0.64278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chAaRH02ihdx"
      },
      "source": [
        "The accuracy of binary RNN using Google word2vec is <font color=red>0.8449</font>\n",
        "\n",
        "The accuracy of binary RNN is using my word2vec model is <font color=red>0.851</font>\n",
        "\n",
        "The accuracy of ternary RNN is using Google word2vec is <font color=red>0.63382</font>\n",
        "\n",
        "The accuracy of ternary RNN using my word2vec model is <font color=red>0.63576</font>\n",
        "\n",
        "The accuracy of binary GRU using Google word2vec is <font color=red>0.8768</font>\n",
        "\n",
        "The accuracy of binary GRU is using my word2vec model is <font color=red>0.86945</font>\n",
        "\n",
        "The accuracy of ternary GRU is using Google word2vec is <font color=red>0.6524</font>\n",
        "\n",
        "The accuracy of ternary GRU using my word2vec model is <font color=red>0.64278</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiKvHIw3jk-O"
      },
      "source": [
        "Samiliar with part 4, the accuracies of binary classification are higher than ternary classification. Besides, the overall accuracy tested by RNN is higher than the performance of FNN."
      ]
    }
  ]
}