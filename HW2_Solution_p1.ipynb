{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb23843",
   "metadata": {},
   "source": [
    "# <center>CSCI544 Homework2 Report</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6aa02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d7fac",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ea952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('amazon_reviews_us_Kitchen_v1_00.tsv',\n",
    "                 sep='\\t',\n",
    "                 usecols=['star_rating','review_body'])\n",
    "# dropping the rows with missing values (eg., Nan)\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78020b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    3124740\n",
       "4.0     731718\n",
       "1.0     426887\n",
       "3.0     349552\n",
       "2.0     241945\n",
       "Name: star_rating, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['star_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6288c7",
   "metadata": {},
   "source": [
    "Random select 250K reviews along with rates, 50K instances per rating score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc0446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = (df[df['star_rating'] == 5]).sample(n=50000, random_state = 3)\n",
    "data_4 = (df[df['star_rating'] == 4]).sample(n=50000, random_state = 3)\n",
    "data_3 = (df[df['star_rating'] == 3]).sample(n=50000, random_state = 3)\n",
    "data_2 = (df[df['star_rating'] == 2]).sample(n=50000, random_state = 3)\n",
    "data_1 = (df[df['star_rating'] == 1]).sample(n=50000, random_state = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a02880",
   "metadata": {},
   "source": [
    "use two datasets *data* and *data_without3* to store the generating data, the latter one does not contain the 3 stars rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ff8e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "# merge two sub dataset and shuffle\n",
    "data = shuffle(pd.concat([data_5, data_4, data_3, data_2, data_1]))\n",
    "data_without3 = shuffle(pd.concat([data_5, data_4, data_2, data_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89d970e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "data_without3 = data_without3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e690d",
   "metadata": {},
   "source": [
    "Relable the 'star_rating', 4 and 5 denote to possible sentiment (class 1) which will be represented by *label 1*, 1 and 2 will be represented by *lable 2* and star 3 will be denoted to *label 3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546beb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    100000\n",
       "2    100000\n",
       "3     50000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = data.star_rating.apply(lambda x: 1 if(x>=4) else (2 if(x<=2) else 3))\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5db27",
   "metadata": {},
   "source": [
    "Relable the 'star_rating', 4 and 5 denote to possible sentiment (class 1) which will be represented by *label 1*, 1 and 2 will be represented by *lable 0* in the *data_without3* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b131e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100000\n",
       "1    100000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_without3['label'] = data_without3.star_rating.apply(lambda x: 1 if(x>=4) else 0)\n",
    "data_without3['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692aa66e",
   "metadata": {},
   "source": [
    "After relabeling, there are 100,000 instances in *label 1* and *label 2*, respectively and 50,000 instances in *label 3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e76e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['review_body','label']]\n",
    "data_without3 = data_without3[['review_body','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ee162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data to local\n",
    "data.to_csv('review_data.csv',index=False)\n",
    "data_without3.to_csv('review_data_without3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27db2906",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad76bbaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice, sturdy. Arrived as promised. Head comes ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yuummmm!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I bought this item because I wanted a healthy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I decided to put the coffee grinder, lid and a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This pattern is stunning and certainly catches...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  label\n",
       "0  Nice, sturdy. Arrived as promised. Head comes ...      1\n",
       "1                                           Yuummmm!      1\n",
       "2  I bought this item because I wanted a healthy ...      1\n",
       "3  I decided to put the coffee grinder, lid and a...      0\n",
       "4  This pattern is stunning and certainly catches...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('review_data.csv')\n",
    "data_without3 = pd.read_csv('review_data_without3.csv')\n",
    "data_without3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59611f4",
   "metadata": {},
   "source": [
    "For the dataset which does not contain the class 3, there are 160,000 items in training set and 40,000 in testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246b03c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000 40000 160000 40000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label = data_without3['label']\n",
    "reviews = data_without3.drop('label',axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, label, random_state=42, test_size=0.2)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213f2db",
   "metadata": {},
   "source": [
    "For the dataset which contain class 3, there are 200,000 items in training set and 50,000 in testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7254b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 50000 200000 50000\n"
     ]
    }
   ],
   "source": [
    "label1 = data['label']\n",
    "reviews1 = data.drop('label',axis=1)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(reviews1, label1, random_state=42, test_size=0.2)\n",
    "print(len(X_train1), len(X_test1), len(y_train1), len(y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "797c0775",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv('bin_label_train.csv',index=False)\n",
    "y_test.to_csv('bin_label_test.csv',index=False)\n",
    "y_train1.to_csv('ter_label_train.csv',index=False)\n",
    "y_test1.to_csv('ter_label_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddbf4c",
   "metadata": {},
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5f2b6",
   "metadata": {},
   "source": [
    "### (a) Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2289a900",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7083ed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'well'\t'excellent'\t0.40\n",
      "'men'\t'women'\t0.77\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('well', 'excellent'),\n",
    "    ('men','women')\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69690d7e",
   "metadata": {},
   "source": [
    "### (b) Train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9201f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f649bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for d in data.review_body:\n",
    "    reviews.append(utils.simple_preprocess(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0c1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences = reviews, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4be68f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'w2v'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060b1b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecb7a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'well'\t'excellent'\t0.22\n",
      "'men'\t'women'\t0.69\n"
     ]
    }
   ],
   "source": [
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, model.wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4edf8",
   "metadata": {},
   "source": [
    "According to the similarity shown in two models mentioned above, it is obviously that pretrained model encoding semantic similarities between words better. I think the smaller the corpus and the stronger the relevance between documents, the generated word2vec model has greater advantages in checking the word similarity in documents, that is, the similarity of similar words is higher. However, due to the small corpus, many words that do not appear cannot be vectored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a3875",
   "metadata": {},
   "source": [
    "# 3. Simple Models\n",
    "\n",
    "In this section, using dataset without class 3 to train perceptron and SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8642",
   "metadata": {},
   "source": [
    "## data cleaning\n",
    "\n",
    "Perform contractions on the reviews.\n",
    "\n",
    "Convert the all reviews into the lower case.\n",
    "\n",
    "Remove the HTML and URLs from the reviews.\n",
    "\n",
    "Remove non-alphabetical characters.\n",
    "\n",
    "Remove the extra tab, Line break, spaces, etc. between the words.\n",
    "\n",
    "The following are examples of cleaning X_train and X_test dataset, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e2bcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "def contractionfunction(s):\n",
    "    words = []\n",
    "    for word in s.split():\n",
    "        words.append(contractions.fix(word))\n",
    "    new_str = ' '.join(words)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f2b627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def data_clean(X):\n",
    "    l = []\n",
    "    for r in X.review_body:\n",
    "        # contraction\n",
    "        r = contractionfunction(r)\n",
    "        # change all letters into lower case\n",
    "        r = r.lower()\n",
    "        # dealing with html\n",
    "        r = re.sub(r'</?\\w+[^>]*>',' ',r)\n",
    "        # dealing with urls\n",
    "        r = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "                   '', r)\n",
    "        # dealing with non-alphabetical characters\n",
    "        r = re.sub(\"[^a-z]+\",' ',r)\n",
    "        # dealing with \\n, \\b, \\r, \\t\n",
    "        r = re.sub(r'\\r|\\n|\\t','',r)\n",
    "        # dealing with ,/./:/extra spaces....\n",
    "        r = re.sub(r'[^\\w\\s]','',r)\n",
    "        # perform contractions on the reviews\n",
    "        l.append(r) \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c30d5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with X_train\n",
    "reviews_train = data_clean(X_train)\n",
    "# dealing with X_test\n",
    "reviews_test = data_clean(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d4a18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with X_train1\n",
    "reviews_train1 = data_clean(X_train1)\n",
    "# dealing with X_test1\n",
    "reviews_test1 = data_clean(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab365ae",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "### Using the same processes in homework1 to remove stop words and perform lemmatization\n",
    "\n",
    "#### remove the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b215e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def rm_stopwords(review):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in review.split(' ') if w not in stop_words]\n",
    "    s = ' '.join(words)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e23174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words in train and test dataset respectively\n",
    "reviews_train_rmstopwords = []\n",
    "reviews_test_rmstopwords = []\n",
    "for review in reviews_train:\n",
    "    reviews_train_rmstopwords.append(rm_stopwords(review))\n",
    "for review in reviews_test:\n",
    "    reviews_test_rmstopwords.append(rm_stopwords(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ffd5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words in train and test dataset respectively\n",
    "reviews_train_rmstopwords1 = []\n",
    "reviews_test_rmstopwords1 = []\n",
    "for review in reviews_train1:\n",
    "    reviews_train_rmstopwords1.append(rm_stopwords(review))\n",
    "for review in reviews_test1:\n",
    "    reviews_test_rmstopwords1.append(rm_stopwords(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59499ecb",
   "metadata": {},
   "source": [
    "#### perform lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4303bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def per_lemmatize(review):\n",
    "    lm = WordNetLemmatizer()\n",
    "    words = [lm.lemmatize(w) for w in review.split(' ')]\n",
    "    s = ' '.join(words)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bae80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train_lemmatize = []\n",
    "reviews_test_lemmatize = []\n",
    "for review in reviews_train_rmstopwords:\n",
    "    reviews_train_lemmatize.append(per_lemmatize(review))\n",
    "for review in reviews_test_rmstopwords:\n",
    "    reviews_test_lemmatize.append(per_lemmatize(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daefdbba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_train_lemmatize_split = []\n",
    "reviews_test_lemmatize_split = []\n",
    "for r in reviews_train_lemmatize:\n",
    "    reviews_train_lemmatize_split.append(r.split(' '))\n",
    "for r in reviews_test_lemmatize:\n",
    "    reviews_test_lemmatize_split.append(r.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4149a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path1 = 'train_datawithout3.json'\n",
    "with open(file_path1,'w') as f:\n",
    "    json.dump(reviews_train_lemmatize_split, f)\n",
    "file_path2 = 'test_datawithout3.json'\n",
    "with open(file_path2,'w') as f:\n",
    "    json.dump(reviews_test_lemmatize_split, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccd96875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with dataset with class 3\n",
    "reviews_train_lemmatize1 = []\n",
    "reviews_test_lemmatize1 = []\n",
    "for review in reviews_train_rmstopwords1:\n",
    "    reviews_train_lemmatize1.append(per_lemmatize(review))\n",
    "for review in reviews_test_rmstopwords1:\n",
    "    reviews_test_lemmatize1.append(per_lemmatize(review))\n",
    "    \n",
    "reviews_train_lemmatize_split1 = []\n",
    "reviews_test_lemmatize_split1 = []\n",
    "for r in reviews_train_lemmatize1:\n",
    "    reviews_train_lemmatize_split1.append(r.split(' '))\n",
    "for r in reviews_test_lemmatize1:\n",
    "    reviews_test_lemmatize_split1.append(r.split(' '))\n",
    "    \n",
    "file_path3 = 'train_data.json'\n",
    "with open(file_path3,'w') as f:\n",
    "    json.dump(reviews_train_lemmatize_split1, f)\n",
    "file_path4 = 'test_data.json'\n",
    "with open(file_path4,'w') as f:\n",
    "    json.dump(reviews_test_lemmatize_split1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4da8c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path1 = 'train_datawithout3.json'\n",
    "file_path2 = 'test_datawithout3.json'\n",
    "\n",
    "reviews_train_lemmatize_split = json.load(open(file_path1))\n",
    "reviews_test_lemmatize_split = json.load(open(file_path2))\n",
    "\n",
    "file_path3 = 'train_data.json'\n",
    "file_path4 = 'test_data.json'\n",
    "\n",
    "reviews_train_lemmatize_split1 = json.load(open(file_path3))\n",
    "reviews_test_lemmatize_split1 = json.load(open(file_path4))\n",
    "\n",
    "y_train = pd.read_csv('bin_label_train.csv')\n",
    "y_test = pd.read_csv('bin_label_test.csv')\n",
    "y_train1 = pd.read_csv('ter_label_train.csv')\n",
    "y_test1 = pd.read_csv('ter_label_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f6a38",
   "metadata": {},
   "source": [
    "## w2v\n",
    "\n",
    "In this w2v section, using TF-IDF, pre-trained model and model trained by myself to vector the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7990dc",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30020182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_tfidf = vectorizer.fit_transform(reviews_train_lemmatize)\n",
    "X_test_tfidf = vectorizer.transform(reviews_test_lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba51fdca",
   "metadata": {},
   "source": [
    "#### Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9e023",
   "metadata": {},
   "source": [
    "Embeding the words using pre-trained model and when there are some words don't appear in the pre-trained model, I just discard them to simplize the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "41173e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embed(review, m):\n",
    "    doc = np.zeros(300)\n",
    "    n = 0 \n",
    "    for r in review:    \n",
    "        if r in m:\n",
    "            n += 1\n",
    "            doc += m[r]\n",
    "    if n>0:\n",
    "        return doc/n\n",
    "    else:\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa10f715",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pretrain = []\n",
    "x_test_pretrain = []\n",
    "for review in reviews_train_lemmatize_split:\n",
    "    x_train_pretrain.append(word_embed(review, wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split:\n",
    "    x_test_pretrain.append(word_embed(review, wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f549f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with dataset with class 3\n",
    "x_train_pretrain1 = []\n",
    "x_test_pretrain1 = []\n",
    "for review in reviews_train_lemmatize_split1:\n",
    "    x_train_pretrain1.append(word_embed(review, wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split1:\n",
    "    x_test_pretrain1.append(word_embed(review, wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ee042",
   "metadata": {},
   "source": [
    "#### My w2v model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be79ee2",
   "metadata": {},
   "source": [
    "Do the same things by using word2vec model trained by myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bacc568",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_my  = []\n",
    "x_test_my = []\n",
    "for review in reviews_train_lemmatize_split:\n",
    "    x_train_my.append(word_embed(review, model.wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split:\n",
    "    x_test_my.append(word_embed(review, model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c66a24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_my1  = []\n",
    "x_test_my1 = []\n",
    "for review in reviews_train_lemmatize_split1:\n",
    "    x_train_my1.append(word_embed(review, model.wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split1:\n",
    "    x_test_my1.append(word_embed(review, model.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2907203",
   "metadata": {},
   "source": [
    "## 3.1 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f93f4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with the label, convert Series to ndarry type\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "y_train1 = y_train1.values\n",
    "y_test1 = y_test1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eda0d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "def metric_measure(y_test_new, y_test_pred):  \n",
    "    a_test = accuracy_score(y_test_new, y_test_pred)\n",
    "    print('accuracy of test set is:',a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6df5ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "#tf-idf\n",
    "clf1 = Perceptron()\n",
    "clf1.fit(X_train_tfidf, y_train)\n",
    "y_test_tfidf_pred = clf1.predict(X_test_tfidf)\n",
    "#pre-trained\n",
    "clf2 = Perceptron()\n",
    "clf2.fit(x_train_pretrain, y_train)\n",
    "y_test_pretrain_pred = clf2.predict(x_test_pretrain)\n",
    "#my trained w2v\n",
    "clf3 = Perceptron()\n",
    "clf3.fit(x_train_my, y_train)\n",
    "y_test_my_pred = clf3.predict(x_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26a4c0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF:\n",
      "accuracy of test set is: 0.865175\n",
      "\n",
      "word2vec-google-news-300:\n",
      "accuracy of test set is: 0.746125\n",
      "\n",
      "my Word2Vec:\n",
      "accuracy of test set is: 0.817275\n"
     ]
    }
   ],
   "source": [
    "print(\"TF_IDF:\")\n",
    "metric_measure(y_test, y_test_tfidf_pred)\n",
    "print(\"\\nword2vec-google-news-300:\")\n",
    "metric_measure(y_test, y_test_pretrain_pred)\n",
    "print(\"\\nmy Word2Vec:\")\n",
    "metric_measure(y_test, y_test_my_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a091b",
   "metadata": {},
   "source": [
    "## 3.2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74e6f92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_svm1 = LinearSVC()\n",
    "clf_svm1.fit(X_train_tfidf, y_train)\n",
    "y_test_tfidf_pred = clf_svm1.predict(X_test_tfidf)\n",
    "#pre-trained\n",
    "clf_svm2 = LinearSVC()\n",
    "clf_svm2.fit(x_train_pretrain, y_train)\n",
    "y_test_pretrain_pred = clf_svm2.predict(x_test_pretrain)\n",
    "#my trained w2v\n",
    "clf_svm3 = LinearSVC()\n",
    "clf_svm3.fit(x_train_my, y_train)\n",
    "y_test_my_pred = clf_svm3.predict(x_test_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78b4135c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_IDF:\n",
      "accuracy of test set is: 0.891875\n",
      "\n",
      "word2vec-google-news-300:\n",
      "accuracy of test set is: 0.818975\n",
      "\n",
      "my Word2Vec:\n",
      "accuracy of test set is: 0.854825\n"
     ]
    }
   ],
   "source": [
    "print(\"TF_IDF:\")\n",
    "metric_measure(y_test, y_test_tfidf_pred)\n",
    "print(\"\\nword2vec-google-news-300:\")\n",
    "metric_measure(y_test, y_test_pretrain_pred)\n",
    "print(\"\\nmy Word2Vec:\")\n",
    "metric_measure(y_test, y_test_my_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57ef3e",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b051697f",
   "metadata": {},
   "source": [
    "## (a) Average Word Vector\n",
    "\n",
    "### 4-(a)-1 Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db3ac0",
   "metadata": {},
   "source": [
    "The following network is the a binary classification FNN with 2 hidden layers, each with 50 and 10 nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29d89db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryNet, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "         # linear layer (n_hidden -> 2) 2 classes\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fddf1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x, y, optimizer):\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 15\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(x) # prep model for training\n",
    "        loss = criterion(y_pred, y)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, loss))\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06005a4c",
   "metadata": {},
   "source": [
    "Training the binary classification FNN using word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ed3ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryNet(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.692728\n",
      "Epoch: 2 \tTraining Loss: 0.679734\n",
      "Epoch: 3 \tTraining Loss: 0.660719\n",
      "Epoch: 4 \tTraining Loss: 0.637461\n",
      "Epoch: 5 \tTraining Loss: 0.608692\n",
      "Epoch: 6 \tTraining Loss: 0.578150\n",
      "Epoch: 7 \tTraining Loss: 0.550201\n",
      "Epoch: 8 \tTraining Loss: 0.527584\n",
      "Epoch: 9 \tTraining Loss: 0.511844\n",
      "Epoch: 10 \tTraining Loss: 0.502622\n",
      "Epoch: 11 \tTraining Loss: 0.500934\n",
      "Epoch: 12 \tTraining Loss: 0.496939\n",
      "Epoch: 13 \tTraining Loss: 0.488979\n",
      "Epoch: 14 \tTraining Loss: 0.484600\n",
      "Epoch: 15 \tTraining Loss: 0.474112\n"
     ]
    }
   ],
   "source": [
    "model_binary1 = BinaryNet()\n",
    "print(model_binary1)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_binary1.parameters(), lr=0.01)\n",
    "x = torch.from_numpy(np.asarray(x_train_pretrain))\n",
    "y = torch.from_numpy(y_train)\n",
    "# Trian model\n",
    "train_model(model_binary1, x.float(),y,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed9ec849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Binary Classification Feedforward Neural Networks by using word2vec-google-news-300 is: 0.7922\n"
     ]
    }
   ],
   "source": [
    "y_pred_pretrain = np.array(model_binary1(torch.from_numpy(np.asarray(x_test_pretrain)).float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_pretrain)):\n",
    "    if y_pred_pretrain[i]==y_test[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Binary Classification Feedforward Neural Networks by using word2vec-google-news-300 is:\", count/40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02e1f9",
   "metadata": {},
   "source": [
    "Training the binary classification FNN using my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19f28d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryNet(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.695326\n",
      "Epoch: 2 \tTraining Loss: 0.650984\n",
      "Epoch: 3 \tTraining Loss: 0.579021\n",
      "Epoch: 4 \tTraining Loss: 0.518114\n",
      "Epoch: 5 \tTraining Loss: 0.484910\n",
      "Epoch: 6 \tTraining Loss: 0.474535\n",
      "Epoch: 7 \tTraining Loss: 0.470757\n",
      "Epoch: 8 \tTraining Loss: 0.458524\n",
      "Epoch: 9 \tTraining Loss: 0.442295\n",
      "Epoch: 10 \tTraining Loss: 0.425144\n",
      "Epoch: 11 \tTraining Loss: 0.412485\n",
      "Epoch: 12 \tTraining Loss: 0.405563\n",
      "Epoch: 13 \tTraining Loss: 0.400923\n",
      "Epoch: 14 \tTraining Loss: 0.396845\n",
      "Epoch: 15 \tTraining Loss: 0.392746\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "model_binary2 = BinaryNet()\n",
    "print(model_binary2)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_binary2.parameters(), lr=0.01)\n",
    "\n",
    "# Trian model\n",
    "train_model(model_binary2, torch.from_numpy(np.asarray(x_train_my)).float(), torch.from_numpy(y_train),optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "432ccc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Binary Classification Feedforward Neural Networks by using my word2vec model is: 0.838425\n"
     ]
    }
   ],
   "source": [
    "y_pred_my = np.array(model_binary2(torch.from_numpy(np.asarray(x_test_my)).float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_my)):\n",
    "    if y_pred_my[i]==y_test[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Binary Classification Feedforward Neural Networks by using my word2vec model is:\", count/40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06d295",
   "metadata": {},
   "source": [
    "### 4-(a)-2 Ternary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "279a8bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_1(model, x, y, optimizer):\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 15\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(x) # prep model for training\n",
    "        loss = criterion(y_pred, y.float())\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, loss))\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71691d7a",
   "metadata": {},
   "source": [
    "The following it the network of ternary classification, with input 300 dimensions and output 4 dimensions (using one-hot to represents these three labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "302fd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class TernaryNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TernaryNet, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        self.fc1 = nn.Linear(300, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "         # linear layer (n_hidden -> 4) 3 classes\n",
    "        self.fc3 = nn.Linear(hidden_2, 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861337f4",
   "metadata": {},
   "source": [
    "Training Ternary classification FNN using word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65d68d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TernaryNet(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.695023\n",
      "Epoch: 2 \tTraining Loss: 0.667239\n",
      "Epoch: 3 \tTraining Loss: 0.641101\n",
      "Epoch: 4 \tTraining Loss: 0.611481\n",
      "Epoch: 5 \tTraining Loss: 0.581244\n",
      "Epoch: 6 \tTraining Loss: 0.555775\n",
      "Epoch: 7 \tTraining Loss: 0.539521\n",
      "Epoch: 8 \tTraining Loss: 0.532070\n",
      "Epoch: 9 \tTraining Loss: 0.528539\n",
      "Epoch: 10 \tTraining Loss: 0.523814\n",
      "Epoch: 11 \tTraining Loss: 0.517178\n",
      "Epoch: 12 \tTraining Loss: 0.509696\n",
      "Epoch: 13 \tTraining Loss: 0.500452\n",
      "Epoch: 14 \tTraining Loss: 0.491634\n",
      "Epoch: 15 \tTraining Loss: 0.484169\n",
      "Epoch: 16 \tTraining Loss: 0.477923\n",
      "Epoch: 17 \tTraining Loss: 0.470929\n",
      "Epoch: 18 \tTraining Loss: 0.465788\n",
      "Epoch: 19 \tTraining Loss: 0.460675\n",
      "Epoch: 20 \tTraining Loss: 0.456317\n",
      "Epoch: 21 \tTraining Loss: 0.452520\n",
      "Epoch: 22 \tTraining Loss: 0.448611\n",
      "Epoch: 23 \tTraining Loss: 0.445076\n",
      "Epoch: 24 \tTraining Loss: 0.441331\n",
      "Epoch: 25 \tTraining Loss: 0.436855\n",
      "Epoch: 26 \tTraining Loss: 0.434801\n",
      "Epoch: 27 \tTraining Loss: 0.431295\n",
      "Epoch: 28 \tTraining Loss: 0.428811\n",
      "Epoch: 29 \tTraining Loss: 0.426331\n",
      "Epoch: 30 \tTraining Loss: 0.424114\n"
     ]
    }
   ],
   "source": [
    "model_ternary1 = TernaryNet()\n",
    "print(model_ternary1)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer1 = torch.optim.Adam(model_ternary1.parameters(), lr=0.01)\n",
    "y_one_hot = torch.nn.functional.one_hot(torch.from_numpy(y_train1),4) \n",
    "# Trian model\n",
    "train_model_1(model_ternary1, torch.from_numpy(np.asarray(x_train_pretrain1)).float(), y_one_hot, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5366b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Ternary Classification Feedforward Neural Networks by using word2vec-google-news-300 is: 0.59092\n"
     ]
    }
   ],
   "source": [
    "y_pred_pretrain = np.array(model_ternary1(torch.from_numpy(np.asarray(x_test_pretrain1)).float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_pretrain)):\n",
    "    if y_pred_pretrain[i]==y_test1[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Ternary Classification Feedforward Neural Networks by using word2vec-google-news-300 is:\", count/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eed648",
   "metadata": {},
   "source": [
    "Training ternary classification FNN using my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96200622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TernaryNet(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.728929\n",
      "Epoch: 2 \tTraining Loss: 0.692136\n",
      "Epoch: 3 \tTraining Loss: 0.649531\n",
      "Epoch: 4 \tTraining Loss: 0.602641\n",
      "Epoch: 5 \tTraining Loss: 0.562062\n",
      "Epoch: 6 \tTraining Loss: 0.528674\n",
      "Epoch: 7 \tTraining Loss: 0.500969\n",
      "Epoch: 8 \tTraining Loss: 0.481685\n",
      "Epoch: 9 \tTraining Loss: 0.469882\n",
      "Epoch: 10 \tTraining Loss: 0.461428\n",
      "Epoch: 11 \tTraining Loss: 0.451995\n",
      "Epoch: 12 \tTraining Loss: 0.440891\n",
      "Epoch: 13 \tTraining Loss: 0.430162\n",
      "Epoch: 14 \tTraining Loss: 0.421607\n",
      "Epoch: 15 \tTraining Loss: 0.415031\n",
      "Epoch: 16 \tTraining Loss: 0.410670\n",
      "Epoch: 17 \tTraining Loss: 0.403926\n",
      "Epoch: 18 \tTraining Loss: 0.396525\n",
      "Epoch: 19 \tTraining Loss: 0.390659\n",
      "Epoch: 20 \tTraining Loss: 0.385260\n",
      "Epoch: 21 \tTraining Loss: 0.382192\n",
      "Epoch: 22 \tTraining Loss: 0.380088\n",
      "Epoch: 23 \tTraining Loss: 0.378036\n",
      "Epoch: 24 \tTraining Loss: 0.375726\n",
      "Epoch: 25 \tTraining Loss: 0.373912\n",
      "Epoch: 26 \tTraining Loss: 0.371732\n",
      "Epoch: 27 \tTraining Loss: 0.369628\n",
      "Epoch: 28 \tTraining Loss: 0.368704\n",
      "Epoch: 29 \tTraining Loss: 0.367161\n",
      "Epoch: 30 \tTraining Loss: 0.365997\n"
     ]
    }
   ],
   "source": [
    "model_ternary2 = TernaryNet()\n",
    "print(model_ternary2)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer1 = torch.optim.Adam(model_ternary2.parameters(), lr=0.01)\n",
    "# Trian model\n",
    "train_model_1(model_ternary2, torch.from_numpy(np.asarray(x_train_my1)).float(), y_one_hot, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab3d8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Ternary Classification Feedforward Neural Networks by using my word2vec model is: 0.6601\n"
     ]
    }
   ],
   "source": [
    "y_pred_my = np.array(model_ternary2(torch.from_numpy(np.asarray(x_test_my1)).float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_my)):\n",
    "    if y_pred_my[i]==y_test1[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Ternary Classification Feedforward Neural Networks by using my word2vec model is:\", count/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60322c",
   "metadata": {},
   "source": [
    "According to the four networks mentioned above, binary classification models perform better than ternary classification models, and compareed with word2vec-google-news-300, the word2vec trained by myself has contributes to a higher accuracy in test set. The accuracy of these four models are concluded below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6542de9",
   "metadata": {},
   "source": [
    "Binary classification + word2vec-google-news-300: <font color=red>0.7922</font>\n",
    "\n",
    "Binary classification + my word2vec: <font color=red>0.8384</font>\n",
    "\n",
    "Ternary classification + word2vec-google-news-300: <font color=red>0.5909</font>\n",
    "\n",
    "Ternary classification + my word2vec: <font color=red>0.6601</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094c089",
   "metadata": {},
   "source": [
    "## 4-(b) First 10 Word2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0290993",
   "metadata": {},
   "source": [
    "Similar with section w2v in 3, and also skip the unappearing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c4fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embed_1(review, m):\n",
    "    doc = []\n",
    "    n = 0 \n",
    "    for r in review:\n",
    "        if r in m:\n",
    "            n += 1\n",
    "            doc.append(m[r])\n",
    "    while n!=10:\n",
    "        doc.append(np.zeros(300))\n",
    "        n += 1\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8d4ff",
   "metadata": {},
   "source": [
    "#### Word embeding by word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04701b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pretrain_10 = []\n",
    "x_test_pretrain_10 = []\n",
    "for review in reviews_train_lemmatize_split:\n",
    "    x_train_pretrain_10.append(word_embed_1(review, wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split:\n",
    "    x_test_pretrain_10.append(word_embed_1(review, wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b62ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with dataset with class 3\n",
    "x_train_pretrain1_10 = []\n",
    "x_test_pretrain1_10 = []\n",
    "for review in reviews_train_lemmatize_split1:\n",
    "    x_train_pretrain1_10.append(word_embed_1(review, wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split1:\n",
    "    x_test_pretrain1_10.append(word_embed_1(review, wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38545a72",
   "metadata": {},
   "source": [
    "#### Word embeding by my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7f21a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_my_10 = []\n",
    "x_test_my_10 = []\n",
    "for review in reviews_train_lemmatize_split:\n",
    "    x_train_my_10.append(word_embed_1(review, model.wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split:\n",
    "    x_test_my_10.append(word_embed_1(review, model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbd10b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with dataset with class 3\n",
    "x_train_my1_10 = []\n",
    "x_test_my1_10 = []\n",
    "for review in reviews_train_lemmatize_split1:\n",
    "    x_train_my1_10.append(word_embed_1(review, model.wv))\n",
    "    \n",
    "for review in reviews_test_lemmatize_split1:\n",
    "    x_test_my1_10.append(word_embed_1(review, model.wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6029b",
   "metadata": {},
   "source": [
    "### 4-(b)-1 Binary Classification\n",
    "\n",
    "The following is the network of a binary classification FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74a0c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNet_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryNet_10, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (3000 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300*10, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "         # linear layer (n_hidden -> 2) 2 classes\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input\n",
    "        x = x.view(-1, 300*10)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3732c05",
   "metadata": {},
   "source": [
    "Training the binary classification FNN using word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b7d082ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryNet_10(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.694531\n",
      "Epoch: 2 \tTraining Loss: 0.676725\n",
      "Epoch: 3 \tTraining Loss: 0.606086\n",
      "Epoch: 4 \tTraining Loss: 0.580015\n",
      "Epoch: 5 \tTraining Loss: 0.642458\n",
      "Epoch: 6 \tTraining Loss: 0.600940\n",
      "Epoch: 7 \tTraining Loss: 0.596777\n",
      "Epoch: 8 \tTraining Loss: 0.547712\n",
      "Epoch: 9 \tTraining Loss: 0.549594\n",
      "Epoch: 10 \tTraining Loss: 0.560075\n",
      "Epoch: 11 \tTraining Loss: 0.552329\n",
      "Epoch: 12 \tTraining Loss: 0.539295\n",
      "Epoch: 13 \tTraining Loss: 0.531621\n",
      "Epoch: 14 \tTraining Loss: 0.533833\n",
      "Epoch: 15 \tTraining Loss: 0.529248\n"
     ]
    }
   ],
   "source": [
    "model_binary3 = BinaryNet_10()\n",
    "print(model_binary3)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.Adam(model_binary3.parameters(), lr=0.01)\n",
    "x = torch.from_numpy(np.asarray(x_train_pretrain_10))\n",
    "y = torch.from_numpy(y_train)\n",
    "# del x_train_pretrain_10\n",
    "# Trian model\n",
    "train_model(model_binary3, x.float(),y ,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "890d814e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Ternary Classification Feedforward Neural Networks by using word2vec-google-news-300 and the fisrt 10 word vectors is: 0.59386\n"
     ]
    }
   ],
   "source": [
    "xtest = torch.from_numpy(np.asarray(x_test_pretrain_10))\n",
    "y_pred_pretrain = np.array(model_binary3(xtest.float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_pretrain)):\n",
    "    if y_pred_pretrain[i]==y_test[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Ternary Classification Feedforward Neural Networks by using word2vec-google-news-300 and the fisrt 10 word vectors is:\", count/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2512638",
   "metadata": {},
   "source": [
    "Training the binary classification FNN with my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "072eed5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryNet_10(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.698209\n",
      "Epoch: 2 \tTraining Loss: 0.731390\n",
      "Epoch: 3 \tTraining Loss: 0.561387\n",
      "Epoch: 4 \tTraining Loss: 0.577182\n",
      "Epoch: 5 \tTraining Loss: 0.567065\n",
      "Epoch: 6 \tTraining Loss: 0.551155\n",
      "Epoch: 7 \tTraining Loss: 0.531357\n",
      "Epoch: 8 \tTraining Loss: 0.516819\n",
      "Epoch: 9 \tTraining Loss: 0.507629\n",
      "Epoch: 10 \tTraining Loss: 0.503787\n",
      "Epoch: 11 \tTraining Loss: 0.504835\n",
      "Epoch: 12 \tTraining Loss: 0.500498\n",
      "Epoch: 13 \tTraining Loss: 0.494070\n",
      "Epoch: 14 \tTraining Loss: 0.487584\n",
      "Epoch: 15 \tTraining Loss: 0.481250\n"
     ]
    }
   ],
   "source": [
    "model_binary4 = BinaryNet_10()\n",
    "print(model_binary4)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_binary4.parameters(), lr=0.01)\n",
    "x = torch.from_numpy(np.asarray(x_train_my_10))\n",
    "y = torch.from_numpy(y_train)\n",
    "# Trian model\n",
    "train_model(model_binary4, x.float(),y ,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6df0463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Ternary Classification Feedforward Neural Networks by using my word2vec model and the fisrt 10 word vectors is: 0.61254\n"
     ]
    }
   ],
   "source": [
    "xtest = torch.from_numpy(np.asarray(x_test_my_10))\n",
    "y_pred_my = np.array(model_binary4(xtest.float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_my)):\n",
    "    if y_pred_my[i]==y_test[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Ternary Classification Feedforward Neural Networks by using my word2vec model and the fisrt 10 word vectors is:\", count/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad55363",
   "metadata": {},
   "source": [
    "### 4-(b)-2 Ternary Classification\n",
    "\n",
    "The following is the network of a ternary classification FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2d623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TernaryNet_10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TernaryNet_10, self).__init__()\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (3000 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(300*10, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "         # linear layer (n_hidden -> 4) 3 classes\n",
    "        self.fc3 = nn.Linear(hidden_2, 4)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the input\n",
    "        x = x.view(-1, 300*10)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77110fc",
   "metadata": {},
   "source": [
    "Training the ternary classification FNN word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaad8727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TernaryNet_10(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_ternary3 = TernaryNet_10()\n",
    "print(model_ternary3)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer1 = torch.optim.Adam(model_ternary3.parameters(), lr=0.01)\n",
    "y_one_hot = torch.nn.functional.one_hot(torch.from_numpy(y_train1),4) \n",
    "x = torch.from_numpy(np.asarray(x_train_pretrain1_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb74055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.685573\n",
      "Epoch: 2 \tTraining Loss: 0.635210\n",
      "Epoch: 3 \tTraining Loss: 0.609810\n",
      "Epoch: 4 \tTraining Loss: 0.596938\n",
      "Epoch: 5 \tTraining Loss: 0.581282\n",
      "Epoch: 6 \tTraining Loss: 0.560111\n",
      "Epoch: 7 \tTraining Loss: 0.545001\n",
      "Epoch: 8 \tTraining Loss: 0.533099\n",
      "Epoch: 9 \tTraining Loss: 0.522376\n",
      "Epoch: 10 \tTraining Loss: 0.510442\n",
      "Epoch: 11 \tTraining Loss: 0.499474\n",
      "Epoch: 12 \tTraining Loss: 0.488090\n",
      "Epoch: 13 \tTraining Loss: 0.478162\n",
      "Epoch: 14 \tTraining Loss: 0.470140\n",
      "Epoch: 15 \tTraining Loss: 0.463374\n"
     ]
    }
   ],
   "source": [
    "# Trian model\n",
    "train_model_1(model_ternary3, x.float(), y_one_hot, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5195ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Ternary Classification Feedforward Neural Networks by using word2vec-google-news-300 and the first 10 word vectors is: 0.54856\n"
     ]
    }
   ],
   "source": [
    "y_pred_pretrain = np.array(model_ternary3(torch.from_numpy(np.asarray(x_test_pretrain1_10)).float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_pretrain)):\n",
    "    if y_pred_pretrain[i]==y_test1[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Ternary Classification Feedforward Neural Networks by using word2vec-google-news-300 and the first 10 word vectors is:\", count/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb655e6",
   "metadata": {},
   "source": [
    "Training the ternary classification FNN with my word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4408904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TernaryNet_10(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Epoch: 1 \tTraining Loss: 0.661970\n",
      "Epoch: 2 \tTraining Loss: 0.780576\n",
      "Epoch: 3 \tTraining Loss: 0.511252\n",
      "Epoch: 4 \tTraining Loss: 0.488810\n",
      "Epoch: 5 \tTraining Loss: 0.491856\n",
      "Epoch: 6 \tTraining Loss: 0.475775\n",
      "Epoch: 7 \tTraining Loss: 0.459634\n",
      "Epoch: 8 \tTraining Loss: 0.451441\n",
      "Epoch: 9 \tTraining Loss: 0.448487\n",
      "Epoch: 10 \tTraining Loss: 0.443971\n",
      "Epoch: 11 \tTraining Loss: 0.434198\n",
      "Epoch: 12 \tTraining Loss: 0.424064\n",
      "Epoch: 13 \tTraining Loss: 0.418814\n",
      "Epoch: 14 \tTraining Loss: 0.415921\n",
      "Epoch: 15 \tTraining Loss: 0.414045\n"
     ]
    }
   ],
   "source": [
    "model_ternary4 = TernaryNet_10()\n",
    "print(model_ternary4)\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer1 = torch.optim.Adam(model_ternary4.parameters(), lr=0.01)\n",
    "x = torch.from_numpy(np.asarray(x_train_my1_10))\n",
    "# Trian model\n",
    "train_model_1(model_ternary4, x.float(), y_one_hot, optimizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0ba5fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Ternary Classification Feedforward Neural Networks by using my word2vec model and the first 10 word vectors is: 0.5936\n"
     ]
    }
   ],
   "source": [
    "y_pred_my = np.array(model_ternary4(torch.from_numpy(np.asarray(x_test_my1_10)).float()).argmax(axis=1))\n",
    "count = 0\n",
    "for i in range(0,len(y_pred_my)):\n",
    "    if y_pred_my[i]==y_test1[i]:\n",
    "        count += 1\n",
    "print(\"accuracy of Ternary Classification Feedforward Neural Networks by using my word2vec model and the first 10 word vectors is:\", count/50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c81d5c",
   "metadata": {},
   "source": [
    "According to the four networks mentioned above, using only the first 10 vectors in the reviews, binary classification models perform better than ternary classification models, and compareed with word2vec-google-news-300, the word2vec trained by myself has contributes to a higher accuracy in test set. The accuracy of these four models are concluded below.\n",
    "\n",
    "Binary classification + word2vec-google-news-300(first 10 vectors): <font color=red>0.5938</font>\n",
    "\n",
    "Binary classification + my word2vec(first 10 vectors): <font color=red>0.6125</font>\n",
    "\n",
    "Ternary classification + word2vec-google-news-300(first 10 vectors): <font color=red>0.5486</font>\n",
    "\n",
    "Ternary classification + my word2vec(first 10 vectors): <font color=red>0.5936</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686061b",
   "metadata": {},
   "source": [
    "After comparing all of these 8 models in section 4, I can find out that although using the average word vectors performs better than using only the first 10 word vector, all of their performances are not better than simple models mentioned in section 3. Only one binary classification which using average word2vec trained by myself has the best accuracy in these 8 models, which is 0.8384. It is similar with the accuracy of Perceptron with my wrod2vec and SVM with my word2vec, and it is higher than perceptron and SVM using word2vec-google-news-300, but lower than those two models using TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb101f8",
   "metadata": {},
   "source": [
    "The part 5 Recurrent Neural Network is in the file CSCI544-HW2_p2.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
